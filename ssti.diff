diff --git a/.github/workflows/semgrep-test.yml b/.github/workflows/semgrep-test.yml
index bc4a0d3..6a966ac 100644
--- a/.github/workflows/semgrep-test.yml
+++ b/.github/workflows/semgrep-test.yml
@@ -36,8 +36,8 @@ jobs:
         id: opengrep-base
         run: |
           echo $PWD
-          /home/runner/.opengrep/cli/v1.4.2/opengrep scan --config "p/default" --sarif-output=opengrep-base.sarif baseline_code
-          echo "SARIF_BASE_FILENAME=opengrep-base.sarif" >> "$GITHUB_OUTPUT"
+          /home/runner/.opengrep/cli/v1.4.2/opengrep scan --config "p/default" --sarif-output=base.sarif baseline_code
+          echo "SARIF_BASE_FILENAME=base.sarif" >> "$GITHUB_OUTPUT"
 
       - name: Checkout PR head
         uses: actions/checkout@v4
@@ -49,16 +49,16 @@ jobs:
         id: opengrep-pr
         run: |
           echo $PWD
-          /home/runner/.opengrep/cli/v1.4.2/opengrep scan --config "p/default" --sarif-output=opengrep-pr.sarif pr_code
-          echo "SARIF_PR_FILENAME=opengrep-pr.sarif" >> "$GITHUB_OUTPUT"
+          /home/runner/.opengrep/cli/v1.4.2/opengrep scan --config "p/default" --sarif-output=pr.sarif pr_code
+          echo "SARIF_PR_FILENAME=pr.sarif" >> "$GITHUB_OUTPUT"
 
       - name: Upload sarif files as artifacts
         uses: actions/upload-artifact@v4
         with:
           name: opengrep-sarifs
           path: |
-            ./opengrep-base.sarif
-            ./opengrep-pr.sarif
+            ./base.sarif
+            ./pr.sarif
 
       # This step reads the SARIF file, filters for high and critical vulnerabilities,
       # and then formats and posts a comment to the pull request.
@@ -66,7 +66,7 @@ jobs:
         # This step will only run if the Semgrep scan produced a SARIF file.
         if: always() && steps.opengrep-base.outputs.SARIF_BASE_FILENAME && steps.opengrep-pr.outputs.SARIF_PR_FILENAME
         run: |
-          RESULTS=$(python3 baseline_code/opengrep_parse.py opengrep-base.sarif opengrep-pr.sarif)
+          RESULTS=$(python3 baseline_code/opengrep_parse.py base.sarif pr.sarif)
           high_severity_findings="VULNS!!"
           comment_body="### Opengrep SAST Scan Results
 
diff --git a/.gitignore b/.gitignore
index 0893a44..791281b 100644
--- a/.gitignore
+++ b/.gitignore
@@ -1 +1,3 @@
 /opengrep-json
+.python-version
+.venv
diff --git a/opengrep_parse.py b/opengrep_parse.py
index e0cfcea..7fb567c 100644
--- a/opengrep_parse.py
+++ b/opengrep_parse.py
@@ -1,86 +1,219 @@
 import json
 import argparse
 import sys
+import os
 
-def create_finding_signature(result):
+try:
+    # Use the correct import based on the library's API
+    from tree_sitter_language_pack import get_parser
+except ImportError:
+    print(
+        "Error: 'tree-sitter-language-pack' not found. Please install it with: pip install tree-sitter tree-sitter-language-pack",
+        file=sys.stderr,
+    )
+    sys.exit(1)
+
+
+def get_function_context_with_tree_sitter(file_path, line_number):
     """
-    Creates a unique, hashable signature for a single Semgrep finding.
+    Parses a source file using tree-sitter to find the enclosing function
+    of a given line number. This is language-aware and highly accurate.
+
+    Args:
+        file_path (str): The path to the source file.
+        line_number (int): The line number of the finding (1-indexed).
+
+    Returns:
+        str: The source code of the enclosing function, or None if not found.
+    """
+    try:
+        # Determine the language from the file extension
+        extension = os.path.splitext(file_path)[1]
+        lang_map = {
+            ".py": "python",
+            ".js": "javascript",
+            ".jsx": "javascript",
+            ".ts": "typescript",
+            ".tsx": "typescript",
+            ".go": "go",
+            ".java": "java",
+            ".c": "c",
+            ".cpp": "cpp",
+            ".cs": "c_sharp",
+            ".rb": "ruby",
+            ".rs": "rust",
+        }
+        lang_name = lang_map.get(extension)
+        if not lang_name:
+            return None  # Unsupported language
+
+        # Use the correct get_parser function
+        print(f"Parsing for language: {lang_name}")
+        parser = get_parser(lang_name)
+        if not parser:
+            return None
+
+        with open(file_path, "rb") as f:  # Read as bytes for tree-sitter
+            source_bytes = f.read()
 
-    The signature is a tuple containing the rule ID, the primary file path,
-    and the finding's message text. This combination is robust enough to
-    uniquely identify a specific vulnerability at a specific code location.
+        tree = parser.parse(source_bytes)
+        root_node = tree.root_node
+
+        # Define what we consider a "function" node for different languages
+        function_node_types = [
+            "function_definition",
+            "function_declaration",
+            "method_definition",
+            "arrow_function",
+            "function_item",
+            "method_declaration",
+        ]
+
+        # Traverse the tree to find the smallest enclosing function node
+        smallest_enclosing_node = None
+
+        def find_node(node):
+            # Check if the current node's range contains the line number
+            if (
+                node.start_point[0] <= line_number - 1
+                and node.end_point[0] >= line_number - 1
+            ):
+                print("Inside the if")
+                if node.type in function_node_types:
+                    smallest_enclosing_node = node
+                    return
+            # Recurse into children to find a more specific (smaller) node
+            for child in node.children:
+                print(f"Searching through new child: {child}")
+                find_node(child)
+
+        find_node(root_node)
+
+        if smallest_enclosing_node:
+            # Return the raw text of the function node
+            print(f"Found smallest node: {smallest_enclosing_node}")
+            return smallest_enclosing_node.text.decode("utf8").strip()
+
+    except Exception:
+        # Catch any errors during parsing (file not found, unsupported lang, etc.)
+        return None
+    return None
+
+
+def get_function_context(result):
+    """
+    Tries to find the enclosing function's context from the SARIF result.
+
+    It first checks 'codeFlows' (from taint-tracking). If unavailable, it falls
+    back to parsing the source file with tree-sitter.
 
     Args:
         result (dict): A 'result' object from a SARIF file.
 
     Returns:
-        tuple: A unique signature for the finding, or None if the result
-               is malformed.
-    """G
+        str: A string representing the function context, or None if not found.
+    """
+    # 1. Try to get context from taint-tracking codeFlows (most reliable)
     try:
-        rule_id = result.get('ruleId')
-        message = result['message']['text']
-        # We use the first location as the primary identifier for the finding's location
-        location = result['locations'][0]['physicalLocation']['artifactLocation']['uri']
-        
-        if not all([rule_id, message, location]):
+        code_flow = result.get("codeFlows", [{}])[0]
+        thread_flow = code_flow.get("threadFlows", [{}])[0]
+        function_location = thread_flow["locations"][0]["location"]["physicalLocation"]
+        function_snippet = (
+            function_location.get("region", {}).get("snippet", {}).get("text")
+        )
+        if function_snippet:
+            return function_snippet.strip()
+    except (KeyError, IndexError):
+        print("Missing key...performing fallback method")
+        pass  # Fallback to tree-sitter method
+
+    # 2. Fallback: Parse the source file from the filesystem with tree-sitter
+    try:
+        physical_location = result["locations"][0]["physicalLocation"]
+        file_path = physical_location["artifactLocation"]["uri"]
+        line_number = physical_location["region"]["startLine"]
+        return get_function_context_with_tree_sitter(file_path, line_number)
+    except (KeyError, IndexError):
+        return None
+
+    return None
+
+
+def create_finding_signature(result):
+    """
+    Creates a unique, hashable signature for a single Semgrep finding.
+
+    The signature is based on the rule, file, the vulnerable code snippet,
+    and the broader function context. This makes the signature highly resilient
+    to line number changes while being specific enough to differentiate issues.
+    """
+    try:
+        rule_id = result.get("ruleId")
+
+        physical_location = result["locations"][0]["physicalLocation"]
+        file_path = physical_location["artifactLocation"]["uri"]
+
+        vulnerable_snippet = (
+            physical_location.get("region", {}).get("snippet", {}).get("text")
+        )
+        if vulnerable_snippet:
+            vulnerable_snippet = vulnerable_snippet.strip()
+
+        function_context = get_function_context(result)
+        context_key = function_context if function_context else vulnerable_snippet
+
+        if not all([rule_id, file_path, context_key, vulnerable_snippet]):
             return None
-            
-        return (rule_id, location, message)
+
+        return (rule_id, file_path, context_key, vulnerable_snippet)
     except (KeyError, IndexError):
-        # Handle cases where the result object doesn't have the expected structure
         return None
 
+
 def parse_sarif_file(sarif_path):
     """
     Parses a SARIF file and extracts all findings, returning them as a set
     of unique signatures and a dictionary mapping signatures to full result objects.
-
-    Args:
-        sarif_path (str): The file path to the SARIF file.
-
-    Returns:
-        tuple: A tuple containing:
-            - set: A set of unique finding signatures.
-            - dict: A dictionary mapping each signature to its full result object.
     """
     try:
-        with open(sarif_path, 'r') as f:
+        with open(sarif_path, "r", encoding="utf-8") as f:
             sarif_data = json.load(f)
     except FileNotFoundError:
         print(f"Error: SARIF file not found at {sarif_path}", file=sys.stderr)
         sys.exit(1)
     except json.JSONDecodeError:
-        print(f"Error: Could not decode JSON from {sarif_path}. Is the file valid?", file=sys.stderr)
+        print(
+            f"Error: Could not decode JSON from {sarif_path}. Is the file valid?",
+            file=sys.stderr,
+        )
         sys.exit(1)
 
     findings = set()
     results_map = {}
-    
-    # SARIF files can have multiple 'runs', so we iterate through them
-    for run in sarif_data.get('runs', []):
-        for result in run.get('results', []):
+
+    for run in sarif_data.get("runs", []):
+        for result in run.get("results", []):
             signature = create_finding_signature(result)
-            if signature:
+            if signature and signature not in results_map:
                 findings.add(signature)
                 results_map[signature] = result
-                
+
     return findings, results_map
 
+
 def main():
     """
-    Main function to compare two SARIF files and identify new findings.
+    s    Main function to compare two SARIF files and identify new findings.
     """
     parser = argparse.ArgumentParser(
         description="Compare two Semgrep SARIF files to find new vulnerabilities."
     )
     parser.add_argument(
-        "base_sarif", 
-        help="Path to the SARIF file from the base branch (e.g., main)."
+        "base_sarif", help="Path to the SARIF file from the base branch (e.g., main)."
     )
     parser.add_argument(
-        "head_sarif", 
-        help="Path to the SARIF file from the head branch (e.g., the PR branch)."
+        "head_sarif",
+        help="Path to the SARIF file from the head branch (e.g., the PR branch).",
     )
     args = parser.parse_args()
 
@@ -92,7 +225,6 @@ def main():
     head_findings, head_results_map = parse_sarif_file(args.head_sarif)
     print(f"Found {len(head_findings)} unique findings in the head scan.")
 
-    # Perform the diff to find new vulnerabilities
     new_finding_signatures = head_findings - base_findings
 
     print("\n--- Differential Scan Results ---")
@@ -103,24 +235,25 @@ def main():
     print(f"🚨 Found {len(new_finding_signatures)} new findings:")
     for i, signature in enumerate(new_finding_signatures, 1):
         result = head_results_map[signature]
-        rule_id = result.get('ruleId')
-        message = result['message']['text']
-        location = result['locations'][0]['physicalLocation']
-        file_path = location['artifactLocation']['uri']
-        line = location['region']['startLine']
+        rule_id = result.get("ruleId")
+        message = result["message"]["text"]
+        location = result["locations"][0]["physicalLocation"]
+        file_path = location["artifactLocation"]["uri"]
+        line = location["region"]["startLine"]
+        snippet = (
+            location.get("region", {}).get("snippet", {}).get("text", "N/A").strip()
+        )
 
-        print("\n" + "="*40)
+        print("\n" + "=" * 40)
         print(f"Finding #{i}")
         print(f"  Rule:      {rule_id}")
-        print(f"  File:      {file_path}")
-        print(f"  Line:      {line}")
+        print(f"  File:      {file_path}:{line}")
         print(f"  Message:   {message}")
-        print("="*40)
-    
-    # Exit with a non-zero status code to indicate that new issues were found,
-    # which can be used to fail a CI/CD pipeline.
+        print(f"  Snippet:   `{snippet}`")
+        print("=" * 40)
+
     sys.exit(1)
 
+
 if __name__ == "__main__":
     main()
-
diff --git a/ssti.go b/ssti.go
index 27e8274..4178421 100644
--- a/ssti.go
+++ b/ssti.go
@@ -87,3 +87,18 @@ func no_match(w http.ResponseWriter, req *http.Request) {
 	}
 	tmpl.Execute(w, user1)
 }
+
+func test_rce(userStr string) {
+	cmd := exec.Command(userStr)
+
+	// Create a buffer to capture the standard output
+	var out bytes.Buffer
+	cmd.Stdout = &out
+	// Run the command
+	err := cmd.Run()
+	if err != nil {
+		log.Fatalf("Command failed with error: %v", err)
+	}
+	// Print the captured output
+	fmt.Printf("Command output:\n%s\n", out.String())
+}
